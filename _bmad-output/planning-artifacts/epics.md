---
stepsCompleted: [1, 2, 3, 4]
status: complete
completedAt: '2026-02-17'
inputDocuments:
  - 'planning-artifacts/prd.md'
  - 'planning-artifacts/architecture.md'
  - 'planning-artifacts/research/technical-langfuse-k8s-deployment-research-2026-02-16.md'
---

# langfuse-k8s-cluster - Epic Breakdown

## Overview

This document provides the complete epic and story breakdown for langfuse-k8s-cluster, decomposing the requirements from the PRD and Architecture into implementable stories.

## Requirements Inventory

### Functional Requirements

FR1: Operator can provision a VPC with public subnets across two availability zones via Terraform
FR2: Operator can provision an EKS cluster with a managed node group (2x t3.medium) in public subnets via Terraform
FR3: EKS cluster provides an OIDC provider for IRSA-based service account authentication
FR4: Operator can access the EKS cluster endpoint publicly for kubectl operations
FR5: Operator can provision an RDS PostgreSQL instance in the same VPC as EKS via Terraform
FR6: RDS is publicly accessible; security group allows port 5432 from EKS nodes and all IPs for dev access
FR7: Operator can provision an S3 bucket for Langfuse event, media, and export storage via Terraform
FR8: An IRSA IAM role grants S3 read/write access to Langfuse pods without static credentials
FR9: RDS and S3 persist data independently of EKS cluster lifecycle (survive teardown)
FR10: Operator can deploy the Langfuse Helm chart to EKS via Terraform Helm provider
FR11: Helm release configures Langfuse to use external RDS (bundled PostgreSQL disabled)
FR12: Helm release configures Langfuse to use external S3 via IRSA (bundled MinIO disabled)
FR13: Helm release deploys bundled ClickHouse and Redis for in-cluster use
FR14: Helm release performs headless initialization creating a default user, organization, and project
FR15: Langfuse secrets (salt, nextauth secret, encryption key) are auto-generated by Terraform
FR16: Workspace 1 (network) outputs are consumable by Workspace 2 (deps) via `tfe_outputs`
FR17: Workspace 1 and 2 outputs are consumable by Workspace 3 (app) via `tfe_outputs`
FR18: Each workspace can be applied and destroyed independently in the correct sequence
FR19: Operator can access Langfuse UI via kubectl port-forward on port 3000
FR20: Operator can access ClickHouse HTTP interface via kubectl port-forward on port 8123
FR21: Operator can access Redis via kubectl port-forward on port 6379
FR22: Langfuse health endpoint (`/api/public/health`) returns 200 when the system is operational
FR23: All user-provided configuration is defined via a single `.env` file
FR24: A `.env.example` template with placeholder values is included in the repository
FR25: `.env` is excluded from version control via `.gitignore`
FR26: No interactive CLI login commands are required — all authentication is via environment variables
FR27: Operator can destroy all infrastructure in reverse workspace order via `terraform destroy`
FR28: Operator can rebuild the full stack from scratch and recover all RDS/S3 data
FR29: Terraform applies are idempotent — re-running produces no unintended changes
FR30: README documents prerequisites (accounts, tooling, environment variables)
FR31: README documents the deploy sequence (apply order, wait times, verification commands)
FR32: README documents teardown sequence and data persistence behavior
FR33: README documents port-forward commands for all accessible services

### NonFunctional Requirements

NFR1: No secrets (AWS keys, TFE token, Langfuse passwords) are committed to version control
NFR2: Auto-generated secrets use cryptographically secure random generation (minimum 32 bytes)
NFR3: RDS is publicly accessible with port 5432 open to all IPs — dev simplicity, protected by auto-generated password
NFR4: S3 access uses IRSA (short-lived tokens) — no static AWS access keys in pods
NFR5: EKS cluster endpoint is publicly accessible (acceptable for dev)
NFR6: Total monthly infrastructure cost does not exceed ~$155
NFR7: No NAT gateway provisioned (saves ~$32/mo)
NFR8: Infrastructure can be fully destroyed to zero ongoing cost when not in use
NFR9: Terraform code uses community modules (`terraform-aws-modules/*`) where available
NFR10: Helm chart version is pinned to a specific release
NFR11: Each Terraform workspace is self-contained with clear inputs/outputs
NFR12: Terraform state is managed by Terraform Cloud — no local state files

### Additional Requirements

**From Architecture:**
- EBS CSI Driver addon must be provisioned in workspace 1 alongside EKS (with dedicated IRSA role)
- TFC workspace names are hardcoded: `langfuse-network`, `langfuse-deps`, `langfuse-app`
- File-per-concern structure: `vpc.tf`, `eks.tf`, `rds.tf`, `s3.tf`, `irsa.tf`, `helm.tf`, `secrets.tf`, `data.tf`
- `values.yaml.tpl` with `templatefile()` interpolation for Helm values
- Variables only for user-configurable values (`aws_region`, `tfc_organization`, admin credentials); everything else hardcoded
- Cross-workspace output contract is a stable API — 7 outputs from ws1, 5 outputs from ws2
- Helm release name must be `langfuse` (chart assumption)
- RDS endpoint format needs port stripping: `split(":", rds_endpoint)[0]`

**From Research:**
- `shadowDatabaseUrl` must be set (empty string acceptable) for Prisma migrations
- `directUrl` should be configured for migration user with longer timeouts
- `AUTH_DISABLE_SIGNUP = true` for dev (blocks public registration)
- Headless init env vars (`LANGFUSE_INIT_*`) are idempotent — safe across redeploys
- `NEXTAUTH_URL` must be set to `http://localhost:3000` for dev
- S3 `batchExport.enabled` must be explicitly set to `true` (disabled by default)
- `forcePathStyle: false` for native AWS S3

### FR Coverage Map

FR1: Epic 1 - VPC with public subnets across two AZs
FR2: Epic 1 - EKS cluster with managed node group
FR3: Epic 1 - OIDC provider for IRSA
FR4: Epic 1 - Public EKS cluster endpoint
FR5: Epic 2 - RDS PostgreSQL instance
FR6: Epic 2 - RDS public accessibility and security group
FR7: Epic 2 - S3 bucket for Langfuse storage
FR8: Epic 2 - IRSA IAM role for S3 access
FR9: Epic 2 - RDS/S3 data persistence independent of EKS
FR10: Epic 3 - Langfuse Helm chart deployment
FR11: Epic 3 - External RDS configuration (bundled PostgreSQL disabled)
FR12: Epic 3 - External S3 via IRSA (bundled MinIO disabled)
FR13: Epic 3 - Bundled ClickHouse and Redis
FR14: Epic 3 - Headless initialization (default user/org/project)
FR15: Epic 3 - Auto-generated Langfuse secrets
FR16: Epic 1 - Workspace 1 outputs consumable by Workspace 2
FR17: Epic 2 - Workspace 1+2 outputs consumable by Workspace 3
FR18: Epic 5 - Independent workspace apply/destroy
FR19: Epic 4 - Langfuse UI port-forward access
FR20: Epic 4 - ClickHouse HTTP port-forward access
FR21: Epic 4 - Redis port-forward access
FR22: Epic 4 - Health endpoint verification
FR23: Epic 6 - Single `.env` file configuration
FR24: Epic 6 - `.env.example` template
FR25: Epic 6 - `.env` excluded from version control
FR26: Epic 6 - Environment-variable-only authentication
FR27: Epic 5 - Destroy in reverse workspace order
FR28: Epic 5 - Rebuild and recover data
FR29: Epic 5 - Idempotent Terraform applies
FR30: Epic 6 - README prerequisites documentation
FR31: Epic 6 - README deploy sequence documentation
FR32: Epic 6 - README teardown documentation
FR33: Epic 6 - README port-forward documentation

## Epic List

### Epic 1: Cloud Network & Compute Platform
Operator can provision a VPC and EKS cluster in AWS, establishing the foundational compute platform for all subsequent workloads. Corresponds to Terraform workspace `langfuse-network`.
**FRs covered:** FR1, FR2, FR3, FR4, FR16

### Epic 2: Persistent Data & Storage Services
Operator can provision RDS PostgreSQL and S3 with IRSA-based secure access, ensuring data persists independently of the cluster lifecycle. Corresponds to Terraform workspace `langfuse-deps`.
**FRs covered:** FR5, FR6, FR7, FR8, FR9, FR17

### Epic 3: Langfuse Application Deployment
Operator can deploy a fully configured, initialized Langfuse instance via Helm, with external RDS/S3, bundled ClickHouse/Redis, auto-generated secrets, and headless initialization. Corresponds to Terraform workspace `langfuse-app`.
**FRs covered:** FR10, FR11, FR12, FR13, FR14, FR15

### Epic 4: Operational Access & Verification
Operator can access all deployed services via port-forward and verify the system is healthy.
**FRs covered:** FR19, FR20, FR21, FR22

### Epic 5: Infrastructure Lifecycle Management
Operator can destroy, rebuild, and re-apply infrastructure reliably with independent workspace operations and idempotent applies.
**FRs covered:** FR18, FR27, FR28, FR29

### Epic 6: Configuration & Documentation
Operator can configure the system via a single `.env` file and follow comprehensive README documentation for deployment, teardown, and access.
**FRs covered:** FR23, FR24, FR25, FR26, FR30, FR31, FR32, FR33

## Epic 1: Cloud Network & Compute Platform

Operator can provision a VPC and EKS cluster in AWS, establishing the foundational compute platform for all subsequent workloads. Corresponds to Terraform workspace `langfuse-network`.

### Story 1.1: Terraform Workspace Scaffold & VPC Provisioning

As an operator,
I want to provision a VPC with public subnets across two availability zones via Terraform Cloud,
So that I have the network foundation for deploying EKS and other AWS resources.

**Acceptance Criteria:**

**Given** the operator has configured `.env` with `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `TFC_TOKEN`, and `TFC_ORGANIZATION`
**When** `terraform init && terraform apply` is run in `terraform/01-network/`
**Then** a VPC is created with public subnets in 2 AZs, no NAT gateway, and DNS hostnames enabled
**And** `providers.tf` configures the TFC backend with workspace name `langfuse-network` and the AWS provider
**And** `variables.tf` defines `aws_region` (default `us-east-1`) and `tfc_organization` (no default)
**And** `vpc.tf` uses `terraform-aws-modules/vpc/aws` (~>6.6) with hardcoded CIDR `10.0.0.0/16` and 2 public subnets
**And** `outputs.tf` exports `vpc_id` and `public_subnet_ids`
**And** Terraform state is stored in Terraform Cloud (no local `.tfstate`)

### Story 1.2: EKS Cluster, Addons & Workspace Outputs

As an operator,
I want to provision an EKS cluster with a managed node group, OIDC provider, and EBS CSI Driver in the VPC,
So that I have a Kubernetes platform ready for workloads and persistent volume support.

**Acceptance Criteria:**

**Given** Story 1.1 has been applied and the VPC exists
**When** `terraform apply` is run in `terraform/01-network/`
**Then** an EKS cluster (version 1.35) is created using `terraform-aws-modules/eks/aws` (~>21.15) in the public subnets
**And** a managed node group with 2x `t3.medium` instances is provisioned
**And** the cluster endpoint is publicly accessible
**And** an OIDC provider is enabled for IRSA-based service account authentication
**And** the EBS CSI Driver addon is installed with a dedicated IRSA role (required for ClickHouse PVC)
**And** `outputs.tf` exports all 7 workspace outputs: `vpc_id`, `public_subnet_ids`, `cluster_name`, `cluster_endpoint`, `cluster_ca_data`, `oidc_provider_arn`, `node_security_group_id`
**And** these outputs are consumable by workspace 2 (`langfuse-deps`) via `tfe_outputs`

## Epic 2: Persistent Data & Storage Services

Operator can provision RDS PostgreSQL and S3 with IRSA-based secure access, ensuring data persists independently of the cluster lifecycle. Corresponds to Terraform workspace `langfuse-deps`.

### Story 2.1: Workspace Scaffold & RDS PostgreSQL Provisioning

As an operator,
I want to provision an RDS PostgreSQL instance in the same VPC as EKS via Terraform Cloud,
So that Langfuse has a persistent relational database that survives cluster teardown.

**Acceptance Criteria:**

**Given** workspace 1 (`langfuse-network`) has been applied and outputs are available
**When** `terraform init && terraform apply` is run in `terraform/02-deps/`
**Then** `providers.tf` configures the TFC backend with workspace name `langfuse-deps` and the AWS provider
**And** `variables.tf` defines `aws_region` (default `us-east-1`) and `tfc_organization` (no default)
**And** `data.tf` reads workspace 1 outputs (`vpc_id`, `public_subnet_ids`, `node_security_group_id`) via `tfe_outputs`
**And** `rds.tf` uses `terraform-aws-modules/rds/aws` (~>7.1) to create a PostgreSQL 16 instance (`db.t4g.micro`)
**And** RDS is publicly accessible with a security group allowing port 5432 from EKS node SG and `0.0.0.0/0`
**And** the database password is auto-generated using `random_password` (32 characters, cryptographically secure)
**And** `deletion_protection` is set to `false` for teardown support
**And** `outputs.tf` exports `rds_endpoint` and `rds_password` (marked sensitive)

### Story 2.2: S3 Bucket for Langfuse Storage

As an operator,
I want to provision an S3 bucket for Langfuse event, media, and export storage,
So that Langfuse has object storage that persists independently of the cluster.

**Acceptance Criteria:**

**Given** Story 2.1 has been applied and the workspace scaffold exists
**When** `terraform apply` is run in `terraform/02-deps/`
**Then** `s3.tf` creates an S3 bucket with a unique name (e.g., `langfuse-storage-{random}`)
**And** `force_destroy` is set to `true` for teardown support
**And** the bucket is in the same region as the EKS cluster
**And** `outputs.tf` exports `s3_bucket_name` and `s3_bucket_region`

### Story 2.3: IRSA Role for S3 Access & Workspace Outputs

As an operator,
I want an IAM role with IRSA trust policy that grants Langfuse pods S3 read/write access without static credentials,
So that the application can securely access S3 using short-lived tokens.

**Acceptance Criteria:**

**Given** Stories 2.1 and 2.2 have been applied, and workspace 1 outputs include `oidc_provider_arn`
**When** `terraform apply` is run in `terraform/02-deps/`
**Then** `irsa.tf` uses `terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks` (~>6.4) to create an IAM role
**And** the role trust policy references the EKS OIDC provider for the `langfuse` namespace and appropriate service account
**And** the role policy grants `s3:GetObject`, `s3:PutObject`, `s3:DeleteObject`, `s3:ListBucket` on the Langfuse S3 bucket
**And** `outputs.tf` exports `irsa_role_arn`
**And** all 5 workspace 2 outputs (`rds_endpoint`, `rds_password`, `s3_bucket_name`, `s3_bucket_region`, `irsa_role_arn`) are consumable by workspace 3 via `tfe_outputs`

## Epic 3: Langfuse Application Deployment

Operator can deploy a fully configured, initialized Langfuse instance via Helm, with external RDS/S3, bundled ClickHouse/Redis, auto-generated secrets, and headless initialization. Corresponds to Terraform workspace `langfuse-app`.

### Story 3.1: Workspace Scaffold, Cross-Workspace Data & Secret Generation

As an operator,
I want the app workspace to consume outputs from workspaces 1 and 2 and auto-generate all Langfuse secrets,
So that the Helm deployment has all required configuration without manual secret management.

**Acceptance Criteria:**

**Given** workspaces 1 and 2 have been applied and their outputs are available
**When** `terraform init && terraform apply` is run in `terraform/03-app/`
**Then** `providers.tf` configures the TFC backend with workspace name `langfuse-app`, the AWS provider, the Helm provider, and the Kubernetes provider (using EKS cluster endpoint and CA data)
**And** `variables.tf` defines `aws_region`, `tfc_organization`, `langfuse_admin_email`, and `langfuse_admin_password`
**And** `data.tf` reads workspace 1 outputs (`cluster_name`, `cluster_endpoint`, `cluster_ca_data`) and workspace 2 outputs (`rds_endpoint`, `rds_password`, `s3_bucket_name`, `s3_bucket_region`, `irsa_role_arn`) via `tfe_outputs`
**And** `secrets.tf` generates three cryptographically secure random strings (minimum 32 bytes each) for `NEXTAUTH_SECRET`, `SALT`, and `ENCRYPTION_KEY`

### Story 3.2: Helm Values Template

As an operator,
I want a comprehensive Helm values template that configures Langfuse for external RDS, external S3 via IRSA, bundled ClickHouse/Redis, and headless initialization,
So that the Helm chart deploys a correctly wired Langfuse instance.

**Acceptance Criteria:**

**Given** Story 3.1 has been applied and all data sources and secrets are available
**When** the `values.yaml.tpl` is rendered via `templatefile()`
**Then** bundled PostgreSQL is disabled and `databaseUrl` points to the external RDS (with port stripped via `split(":", rds_endpoint)[0]`)
**And** `shadowDatabaseUrl` is set to empty string and `directUrl` is configured for Prisma migrations
**And** bundled MinIO is disabled and S3 is configured with `bucket`, `region`, `forcePathStyle: false`, and `batchExport.enabled: true`
**And** the Langfuse service account is annotated with the IRSA role ARN for S3 access
**And** bundled ClickHouse is enabled with default settings
**And** bundled Redis is enabled with default settings
**And** `NEXTAUTH_URL` is set to `http://localhost:3000`
**And** `AUTH_DISABLE_SIGNUP` is set to `true`
**And** `NEXTAUTH_SECRET`, `SALT`, and `ENCRYPTION_KEY` are injected from generated secrets
**And** headless init env vars (`LANGFUSE_INIT_ORG_ID`, `LANGFUSE_INIT_ORG_NAME`, `LANGFUSE_INIT_PROJECT_NAME`, `LANGFUSE_INIT_USER_EMAIL`, `LANGFUSE_INIT_USER_NAME`, `LANGFUSE_INIT_USER_PASSWORD`) are configured

### Story 3.3: Helm Release Deployment

As an operator,
I want to deploy the Langfuse Helm chart to EKS via the Terraform Helm provider,
So that a running Langfuse instance is created with a default user, organization, and project.

**Acceptance Criteria:**

**Given** Stories 3.1 and 3.2 are in place
**When** `terraform apply` is run in `terraform/03-app/`
**Then** `helm.tf` deploys a `helm_release` named `langfuse` using chart `langfuse/langfuse` (~>1.5) from repository `https://langfuse.github.io/langfuse-k8s`
**And** the Helm chart version is pinned to `1.5.19`
**And** the release uses `values.yaml.tpl` rendered via `templatefile()` with all interpolated variables
**And** the Langfuse web pod starts and creates the default user, organization, and project via headless initialization
**And** headless init is idempotent — re-applying produces no duplicate resources
**And** the release is deployed in the `langfuse` namespace

## Epic 4: Operational Access & Verification

Operator can access all deployed services via port-forward and verify the system is healthy.

### Story 4.1: Service Access via Port-Forward

As an operator,
I want to access Langfuse UI, ClickHouse, and Redis via kubectl port-forward,
So that I can use and inspect all deployed services from my local machine.

**Acceptance Criteria:**

**Given** Epic 3 has been applied and all pods are running
**When** the operator runs `kubectl port-forward svc/langfuse-web 3000:3000`
**Then** the Langfuse UI is accessible at `http://localhost:3000`
**And** when `kubectl port-forward svc/langfuse-clickhouse 8123:8123` is run, ClickHouse HTTP interface is accessible at `http://localhost:8123`
**And** when `kubectl port-forward svc/langfuse-redis-master 6379:6379` is run, Redis is accessible at `localhost:6379`
**And** these commands are documented in the README with exact service names and ports

### Story 4.2: Health Endpoint Verification

As an operator,
I want to verify the Langfuse system is fully operational by checking the health endpoint,
So that I can confirm the deployment was successful and all components are connected.

**Acceptance Criteria:**

**Given** the Langfuse UI is accessible via port-forward on port 3000
**When** the operator runs `curl http://localhost:3000/api/public/health`
**Then** the response status code is `200` and indicates the system is operational
**And** this verification command is documented in the README as the final deploy verification step

## Epic 5: Infrastructure Lifecycle Management

Operator can destroy, rebuild, and re-apply infrastructure reliably with independent workspace operations and idempotent applies.

### Story 5.1: Independent Workspace Destroy & Teardown Sequence

As an operator,
I want to destroy all infrastructure in reverse workspace order and confirm each workspace can be destroyed independently,
So that I can reduce cost to zero when the environment is not in use.

**Acceptance Criteria:**

**Given** all three workspaces have been applied and the full stack is running
**When** the operator runs `terraform destroy` in `terraform/03-app/`, then `terraform/02-deps/`, then `terraform/01-network/`
**Then** each workspace destroys cleanly without errors
**And** workspace 3 can be destroyed independently while workspaces 1 and 2 remain intact
**And** workspace 2 can be destroyed independently while workspace 1 remains intact
**And** after full teardown, no AWS resources remain (zero ongoing cost)
**And** RDS and S3 data persists only if workspace 2 is not destroyed (FR9)
**And** the teardown sequence and data persistence behavior are documented in the README

### Story 5.2: Idempotent Apply & Full Stack Rebuild

As an operator,
I want to rebuild the full stack from scratch and confirm that re-applying any workspace produces no unintended changes,
So that I can recover from teardown and trust that Terraform operations are safe to repeat.

**Acceptance Criteria:**

**Given** the full stack has been destroyed (or is being built fresh)
**When** the operator applies workspaces in order: `01-network`, `02-deps`, `03-app`
**Then** the full stack is rebuilt successfully and Langfuse is operational
**And** if RDS and S3 were not destroyed, all previous data is recovered automatically
**And** re-running `terraform apply` on any already-applied workspace produces no changes (idempotent)
**And** the rebuild sequence and data recovery behavior are documented in the README

## Epic 6: Configuration & Documentation

Operator can configure the system via a single `.env` file and follow comprehensive README documentation for deployment, teardown, and access.

### Story 6.1: Environment Configuration Files

As an operator,
I want a single `.env` file for all user-provided configuration with a `.env.example` template and `.gitignore` exclusion,
So that I can configure the entire stack without editing Terraform files and without risking secret exposure.

**Acceptance Criteria:**

**Given** the operator has cloned the repository
**When** the operator copies `.env.example` to `.env` and fills in their values
**Then** `.env.example` contains placeholder values for all required variables: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `TFC_TOKEN`, `TFC_ORGANIZATION`, `AWS_DEFAULT_REGION`, `LANGFUSE_ADMIN_EMAIL`, `LANGFUSE_ADMIN_NAME`, `LANGFUSE_ADMIN_PASSWORD`
**And** `.env` is listed in `.gitignore` and never committed to version control
**And** no interactive CLI login commands (e.g., `aws configure`, `terraform login`) are required — all authentication is via environment variables
**And** the `.env` file is the single source of truth for all user-configurable values

### Story 6.2: Comprehensive README Documentation

As an operator,
I want a README that documents prerequisites, deploy sequence, teardown, and service access,
So that I (or any operator) can deploy, manage, and tear down the full stack by following the documentation.

**Acceptance Criteria:**

**Given** the operator has read the README
**When** they follow the documented steps
**Then** the **Prerequisites** section lists required accounts (AWS, Terraform Cloud), tooling (`terraform`, `kubectl`, `helm`), and environment variables
**And** the **Deploy** section documents the apply order (`01-network` → `02-deps` → `03-app`), expected wait times, and verification commands
**And** the **Teardown** section documents the reverse destroy order (`03-app` → `02-deps` → `01-network`) and explains data persistence behavior (RDS/S3 survive if ws2 is kept)
**And** the **Access** section documents port-forward commands for Langfuse UI (3000), ClickHouse (8123), and Redis (6379) with exact service names
**And** the health check command (`curl http://localhost:3000/api/public/health`) is documented as the final verification step
